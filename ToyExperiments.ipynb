{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sh3rLock3d/MET-P-SVGD/blob/main/ToyExperiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6lbnWXg1j5e"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ehfO-SKvewiJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import autograd\n",
        "import altair as alt\n",
        "alt.data_transformers.enable('default', max_rows=None)\n",
        "import pandas as pd\n",
        "import math\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJJsnk6Z1quI"
      },
      "source": [
        "# Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7bKliL2SezLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a1be895-5927-4ca0-c30f-6c90d3628e8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DfMAF2m1u1H"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6hrpQy2Te3g5"
      },
      "outputs": [],
      "source": [
        "def get_density_chart(P, d=7.0, step=0.1, dist_type=None):\n",
        "    \"\"\"\n",
        "    Given a probability distribution, return a density chart (Heatmap)\n",
        "    Inputs:\n",
        "        P: Probability distribution\n",
        "        d: A value used to bound the meshgrid\n",
        "        step: A value used in the arange method to create the meshgrid\n",
        "    Outputs:\n",
        "        chart: Altair object corresponding to a density plot\n",
        "    \"\"\"\n",
        "    xv, yv = torch.meshgrid([torch.arange(-d, d, step), torch.arange(-d, d, step)])\n",
        "    pos_xy = torch.cat((xv.unsqueeze(-1), yv.unsqueeze(-1)), dim=-1)\n",
        "    p_xy = P.log_prob(pos_xy.to(device)).exp().unsqueeze(-1).cpu()\n",
        "\n",
        "\n",
        "    df = torch.cat([pos_xy, p_xy], dim=-1).numpy()\n",
        "    df = pd.DataFrame({\n",
        "        'x': df[:, :, 0].ravel(),\n",
        "        'y': df[:, :, 1].ravel(),\n",
        "        'p': df[:, :, 2].ravel(),})\n",
        "\n",
        "    chart = alt.Chart(df).mark_point().encode(\n",
        "\n",
        "        x=alt.X('x:Q', axis=alt.Axis(title='', labelFontSize=20, tickSize=12)),\n",
        "        y=alt.Y('y:Q', axis=alt.Axis(title='', labelFontSize=20, tickSize=12)),\n",
        "        color=alt.Color('p:Q', scale=alt.Scale(scheme='lightorange')),\n",
        "        tooltip=['x','y','p']).properties(\n",
        "        width=220,\n",
        "        height=190\n",
        "    )\n",
        "\n",
        "\n",
        "    return chart\n",
        "\n",
        "\n",
        "\n",
        "def get_particles_chart(X, masked = None, X_svgd=None):\n",
        "    \"\"\"\n",
        "    Given a set of points, return a scatter plot\n",
        "    Inputs:\n",
        "        X: Final positions of the particles after applying svgd\n",
        "        X_svgd: Intermidiate position of the particles while applying svgd. If None do not add them to the plot\n",
        "    Outputs:\n",
        "        chart: Altair object corresponding to a scatter plot\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame({\n",
        "        'x': X[:, 0],\n",
        "        'y': X[:, 1],})\n",
        "\n",
        "    chart = alt.Chart(df).mark_circle(color='black').encode(x='x:Q',y='y:Q')\n",
        "\n",
        "    if masked is not None:\n",
        "      df_masked =pd.DataFrame({\n",
        "        'x': masked[:, 0],\n",
        "        'y': masked[:, 1],})\n",
        "      chart += alt.Chart(df_masked).mark_circle(shape = 'cross', color = 'red').encode(x='x:Q',y='y:Q')\n",
        "\n",
        "    if X_svgd is not None:\n",
        "        for i in range(np.shape(X_svgd)[1]):\n",
        "            df_trajectory = pd.DataFrame({'x': X_svgd[:,i,0],'y': X_svgd[:,i,1],})\n",
        "            chart += alt.Chart(df_trajectory).mark_line().mark_circle(color='green').encode(x='x:Q',y='y:Q')\n",
        "\n",
        "    return chart\n",
        "\n",
        "\n",
        "def filter(results, constraints):\n",
        "    \"\"\"\n",
        "    Given a dictionary of experiments results, search it given a list of constraints\n",
        "    Inputs:\n",
        "        results: A dictionary of experimental results\n",
        "        constraints: A list of configuration constraints\n",
        "    Outputs:\n",
        "        Elements based on the input configurations of experiments\n",
        "    \"\"\"\n",
        "    configs = []\n",
        "    for i in range(len(results['configs'])):\n",
        "        check_bool = True\n",
        "        for j in range(len(results['configs'][i])):\n",
        "            if constraints[j] == '*':\n",
        "                continue\n",
        "            if results['configs'][i][j] != constraints[j]:\n",
        "                check_bool = False\n",
        "                break\n",
        "        if check_bool:\n",
        "            configs.append(i)\n",
        "\n",
        "    return np.array(results['sampler_entr_svgd'])[configs].tolist(), np.array(results['gt_entr'])[configs].tolist(), np.array(results['charts_svgd'])[configs].tolist(), np.array(results['init_chart'])[configs].tolist()\n",
        "\n",
        "def figure_4c(results, mu, sigma, figure_name='figure_4c'):\n",
        "    \"\"\"\n",
        "    Create figure 4c in the paper\n",
        "    Inputs:\n",
        "        results: Dictionary containing all the saved data\n",
        "        mu: Mean of the initial distribution\n",
        "        sigma: Standard deviation of the initial distribution\n",
        "        figure_name: Figure name\n",
        "    \"\"\"\n",
        "\n",
        "    x_values = ['0.1', '1', '3', '5', '7', '10', '100']\n",
        "    entropies, gt_entr, charts, init_charts = filter(results, ['*', '*', '*', '*', 200, 200, '*' , mu, sigma])\n",
        "    dis_200s_200p = entropies\n",
        "    entropies, gt_entr, charts, init_charts = filter(results, ['*', '*', '*', '*', 100, 100, '*' , mu, sigma])\n",
        "    dis_100s_100p = entropies\n",
        "    entropies, gt_entr, charts, init_charts = filter(results, ['*', '*', '*', '*', 10, 20, '*' , mu, sigma])\n",
        "    dis_20s_10p = entropies\n",
        "    entropies, gt_entr, charts, init_charts = filter(results, ['*', '*', '*', '*', 20, 20, '*' , mu, sigma])\n",
        "    dis_20s_20p = entropies\n",
        "\n",
        "    plt.rcParams.update({'font.size': 18})\n",
        "    fig = plt.figure(figsize=(8, 5))\n",
        "    plt.scatter(x_values, dis_200s_200p, marker=\"o\", label=\"200\", s=60 )\n",
        "    plt.scatter(x_values, dis_100s_100p, marker=\"s\", label=\"100\", s=60 )\n",
        "    plt.scatter(x_values, dis_20s_20p, marker=\"P\", label=\"20\", s=60 )\n",
        "    plt.scatter(x_values, dis_20s_10p, marker=\"*\", label=\"10\", s=60 )\n",
        "    plt.axhline(y = np.mean(gt_entr), color = 'g', linestyle = 'dotted')\n",
        "    plt.legend(title=\"# particles\", loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "    plt.xlabel(\"Kernel variance\")\n",
        "    plt.ylabel(r\"$H(q^L)$\")\n",
        "    plt.ylim(0, 5)\n",
        "    plt.savefig('./' + figure_name + '.pdf', bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def figure_4b(results, dims, expr_name, x_label, x_ticks, results_tmp=None, results_200=None, resutls_500=None):\n",
        "    \"\"\"\n",
        "    Create figure 4b from the paper\n",
        "    Inputs:\n",
        "        results: Dictionary containing all the results' data\n",
        "        dims: A list containing the dimensions we want to print\n",
        "        expr_name: Experiment name\n",
        "        x_label: X axis label\n",
        "        x_ticks: X axis ticks\n",
        "    \"\"\"\n",
        "\n",
        "    for dim in dims:\n",
        "        filtered_results = [filter(results[i][expr_name], [dim, '*', '*', '*', 10, 20, '*' , 0, 6]) for i in range(len(results))]\n",
        "        gmm1_20s_10p = [filtered_results[i][0] for i in range(len(filtered_results))]\n",
        "\n",
        "        if results_tmp == None:\n",
        "            filtered_results = [filter(results[i][expr_name], [dim, '*', '*', '*', 10, 10, '*' , 0, 6]) for i in range(len(results))]\n",
        "        else:\n",
        "            filtered_results = [filter(results_tmp[i][expr_name], [dim, '*', '*', '*', 10, 10, '*' , 0, 6]) for i in range(len(results_tmp))]\n",
        "        gmm1_10s_10p = [filtered_results[i][0] for i in range(len(filtered_results))]\n",
        "        if results_200 == None:\n",
        "            filtered_results = [filter(results[i][expr_name], [dim, '*', '*', '*', 200, 200, '*' , 0, 6]) for i in range(len(results))]\n",
        "        else:\n",
        "            filtered_results = [filter(results_200[i][expr_name], [dim, '*', '*', '*', 200, 200, '*' , 0, 6]) for i in range(len(results_200))]\n",
        "        gmm1_200s_200p = [filtered_results[i][0] for i in range(len(filtered_results))]\n",
        "\n",
        "        if results_500 == None:\n",
        "            filtered_results = [filter(results[i][expr_name], [dim, '*', '*', '*', 200, 500, '*' , 0, 6]) for i in range(len(results))]\n",
        "        else:\n",
        "            filtered_results = [filter(resutls_500[i][expr_name], [dim, '*', '*', '*', 200, 200, '*' , 0, 6]) for i in range(len(resutls_500))]\n",
        "        gmm1_500s_200p = [filtered_results[i][0] for i in range(len(filtered_results))]\n",
        "\n",
        "\n",
        "        raw_mean = [list(np.nanmean(gmm1_20s_10p, axis=0)), list(np.nanmean(gmm1_10s_10p, axis=0)), list(np.nanmean(gmm1_200s_200p, axis=0)), list(np.nanmean(gmm1_500s_200p, axis=0))]\n",
        "        raw_std = [list(np.nanstd(gmm1_20s_10p, axis=0)), list(np.nanstd(gmm1_10s_10p, axis=0)), list(np.nanstd(gmm1_200s_200p, axis=0)), list(np.nanstd(gmm1_500s_200p, axis=0))]\n",
        "        colors = ['r', 'g', 'b', 'k']\n",
        "        labels = ['#(p,s): (10,20) ', '#(p,s): (10,10) ', '#(p,s): (200,200) ', '#(p,s): (200,500) ']\n",
        "        plt.figure(figsize=(8, 6), dpi=150)\n",
        "        plt.xlabel(x_label)\n",
        "        plt.ylabel(\"Entropy\")\n",
        "        for i in range(len(raw_mean)):\n",
        "            plt.errorbar(np.array([0.0, 1.0, 2.0, 3.0]), raw_mean[i], yerr=raw_std[i], color=colors[i], marker='o', label=labels[i])\n",
        "        plt.xticks(np.array([0.0, 1.0, 2.0, 3.0]), np.array(x_ticks))\n",
        "        plt.xlim([-0.5, 3.5])\n",
        "        plt.title('dim_' + str(dim))\n",
        "        plt.legend()\n",
        "        plt.savefig('./' + expr_name + '_' + str(dim) + '.png', bbox_inches='tight')\n",
        "\n",
        "def stID_chart(x):\n",
        "    df = pd.DataFrame({\n",
        "      'step': range(len(x)),\n",
        "      'stein identity': x,})\n",
        "\n",
        "    chart = alt.Chart(df).mark_line().encode(x='step:Q',y='stein identity:Q')\n",
        "    return chart\n",
        "\n",
        "# def entropies_chart(ents, gt_ent):\n",
        "#   ent1, ent2 = ents[0], ents[1]\n",
        "#   df = pd.DataFrame({\n",
        "#       'svgd_step':range(len(ent1)),\n",
        "#       'entropy1':ent1,\n",
        "#       'entropy2':ent2})\n",
        "#   ## gt entropy\n",
        "#   plt.axhline(y = gt_entr, color='r', linestyle='-', label = 'gt_entropy')\n",
        "#   # line 1\n",
        "#   plt.plot(df['svgd_step'], df['entropy1'], label = 'line 1', color = '#006400')\n",
        "#   #line 2\n",
        "#   plt.plot(df['svgd_step'], df['entropy2'], label = 'line 2', color = '#00CED1')\n",
        "#   # add extra axis for the stein id\n",
        "#   # plt.plot(stein_id_0['svgd_step'], stein_id_0['stein identity'], color= '#0000FF')\n",
        "#   plt.legend()\n",
        "\n",
        "#   plt.xlabel('itr')\n",
        "#   plt.ylabel('entropies')\n",
        "\n",
        "  # plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J73Y-XM10MP"
      },
      "source": [
        "# Kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GPYcD8BSNb_F"
      },
      "outputs": [],
      "source": [
        "class RBF:\n",
        "    \"\"\"\n",
        "    Radial basis funtion kernel (https://en.wikipedia.org/wiki/Radial_basis_function)\n",
        "    Inputs:\n",
        "        sigma: Kernel standard deviation\n",
        "        num_particles: Number of particles\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma, num_particles):\n",
        "        self.sigma = sigma\n",
        "        self.num_particles = num_particles\n",
        "\n",
        "    def forward(self, input_1, input_2):\n",
        "        \"\"\"\n",
        "        Given two tensors of particles, return the matrix of distances between each particle\n",
        "        Inputs:\n",
        "            input_1: Particles coordinates\n",
        "            input_2: Particles coordinates\n",
        "        Outputs:\n",
        "            kappa: RBF matrix\n",
        "            diff: Signed distance\n",
        "            h: Kernel variance\n",
        "            kappa_grad: Derivative of rbf kernel\n",
        "            gamma: 1/2*sigma**2\n",
        "        \"\"\"\n",
        "        # Check if the inputs have the same size in the last two dimensions\n",
        "        assert input_2.size()[-2:] == input_1.size()[-2:]\n",
        "\n",
        "        # Compute the difference between each particle\n",
        "        diff = input_1.unsqueeze(-2) - input_2.unsqueeze(-3)\n",
        "        # Square the difference and sum over the particle's dimensions\n",
        "        dist_sq = diff.pow(2).sum(-1)\n",
        "        dist_sq = dist_sq.unsqueeze(-1)\n",
        "\n",
        "        if self.sigma == \"mean\":\n",
        "            # Estimate the kernel variance using the mean of all the particles distances\n",
        "            median_sq = torch.mean(dist_sq.detach().reshape(-1, self.num_particles*self.num_particles), dim=1)\n",
        "            median_sq = median_sq.unsqueeze(1).unsqueeze(1)\n",
        "            h = median_sq / (2 * np.log(self.num_particles + 1.))\n",
        "            sigma = torch.sqrt(h)\n",
        "        elif self.sigma == \"forth\":\n",
        "            # Estimate the kernel variance using the mean devided by two (one forth) of all the particles distances\n",
        "            median_sq = 0.5 * torch.mean(dist_sq.detach().reshape(-1, self.num_particles*self.num_particles), dim=1)\n",
        "            median_sq = median_sq.unsqueeze(1).unsqueeze(1)\n",
        "            h = median_sq / (2 * np.log(self.num_particles + 1.))\n",
        "            sigma = torch.sqrt(h)\n",
        "        elif self.sigma == \"median\":\n",
        "            # Estimate the kernel variance using the median of all the particles distances\n",
        "            median_sq = torch.median(dist_sq.detach().reshape(-1, self.num_particles*self.num_particles), dim=1)[0]\n",
        "            median_sq = median_sq.unsqueeze(1).unsqueeze(1)\n",
        "            h = median_sq / (2 * np.log(self.num_particles + 1.))\n",
        "            sigma = torch.sqrt(h)\n",
        "        else:\n",
        "            # Setting the kernel variance from the input\n",
        "            sigma = self.sigma\n",
        "            h = None\n",
        "\n",
        "        gamma = 1.0 / (1e-8 + 2 * sigma**2)\n",
        "\n",
        "        kappa = (-gamma * dist_sq).exp()\n",
        "        # Computing the gradient of the kernel over the particles\n",
        "        kappa_grad = -2. * (diff * gamma) * kappa\n",
        "        return kappa.squeeze(), diff, h, kappa_grad, gamma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a99nEtVNb_G"
      },
      "source": [
        "# GMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g03T7aFcNb_G"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GMMDist(object):\n",
        "    \"\"\"\n",
        "    Gaussian Mixture Model\n",
        "    Inputs:\n",
        "        dim: Number of dimensions of the random variable\n",
        "        n_gmm: Number of mixtures\n",
        "        sigma: Mixutures' standard diviation\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, n_gmm, sigma):\n",
        "        def _compute_mu(i):\n",
        "            \"\"\"\n",
        "            Compute the mean of each mixture\n",
        "            Inputs:\n",
        "                i: Index of the mixture\n",
        "            Outputs:\n",
        "                The mean of each dimension of the random variable\n",
        "            \"\"\"\n",
        "            return 4 * torch.Tensor([[torch.tensor(i * math.pi / (n_gmm/2)).sin(),torch.tensor(i * math.pi / (n_gmm/2)).cos()] + [torch.rand((1,)) * 2 - 1 for _ in range(dim - 2)]])\n",
        "\n",
        "        # Generate a list of equal probabilities between the mixtures\n",
        "        self.mix_probs = (1.0/n_gmm) * torch.ones(n_gmm).to(device)\n",
        "        # Compute the means\n",
        "        self.means = torch.cat([_compute_mu(i) for i in range(n_gmm)], dim=0).to(device)\n",
        "        # Compute the standard deviations\n",
        "        self.sigma = sigma\n",
        "        self.std = torch.stack([torch.ones(dim).to(device) * self.sigma for i in range(len(self.mix_probs))], dim=0)\n",
        "\n",
        "\n",
        "    def sample(self, n):\n",
        "        \"\"\"\n",
        "        Sample a tensor of particles\n",
        "        Inputs:\n",
        "            n: Number of particles\n",
        "        Outputs:\n",
        "            A tensor of particles that follows this distribution\n",
        "        \"\"\"\n",
        "        mix_idx = torch.multinomial(self.mix_probs, n[0], replacement=True)\n",
        "        means = self.means[mix_idx]\n",
        "        stds = self.std[mix_idx]\n",
        "        return torch.randn_like(means) * stds + means\n",
        "\n",
        "    def log_prob(self, samples):\n",
        "        \"\"\"\n",
        "        Given a tensor of particles, compute their log probability with respect to the current distribution\n",
        "        (the mixtures are independent)\n",
        "        Inputs:\n",
        "            samples: A tensor of particles\n",
        "        Outputs:\n",
        "            A tensor of log probabilites\n",
        "        \"\"\"\n",
        "        logps = []\n",
        "        for i in range(len(self.mix_probs)):\n",
        "            logps.append((-((samples.to(device) - self.means[i]) ** 2).sum(dim=-1) / (2 * self.sigma ** 2) - 0.5 * np.log(\n",
        "                2 * np.pi * self.sigma ** 2)) + self.mix_probs[i].log())\n",
        "        logp = torch.logsumexp(torch.stack(logps, dim=0), dim=0)\n",
        "        return logp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNVtrc_V15JL"
      },
      "source": [
        "# Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MFRnfC_6e36w"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Optim():\n",
        "    \"\"\"\n",
        "    Optimizer Class\n",
        "    Inputs:\n",
        "        lr: Learning rate\n",
        "    \"\"\"\n",
        "    def __init__(self, lr=None):\n",
        "        self.m_dx, self.v_dx = 0, 0\n",
        "        self.lr = lr\n",
        "\n",
        "    def step(self,x, dx):\n",
        "        \"\"\"\n",
        "        Gradient Ascent\n",
        "        Inputs:\n",
        "            x: A tensor of particles\n",
        "            dx: The update vector\n",
        "        Outputs:\n",
        "            x: Updated particles\n",
        "        \"\"\"\n",
        "        dx = dx.view(x.size())\n",
        "        x = x + self.lr * dx\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKJMc3XC14PX"
      },
      "source": [
        "# Entropy Computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "t5IgmzBKe39j"
      },
      "outputs": [],
      "source": [
        "class Entropy():\n",
        "    \"\"\"\n",
        "    Entropy Class\n",
        "    Inputs:\n",
        "        P: GMM Object\n",
        "        K: RBF Kernel\n",
        "        optimizer: Gradient Ascent optimizer\n",
        "        num_particles: Number of particles\n",
        "        particles_dim: Particles' number of dimensions\n",
        "    \"\"\"\n",
        "    def __init__(self, P, K, optimizer, num_particles, particles_dim):\n",
        "        self.P = P\n",
        "        self.optim = optimizer\n",
        "        self.num_particles = num_particles\n",
        "        self.particles_dim = particles_dim\n",
        "        self.K = K\n",
        "\n",
        "        # Mask parameters\n",
        "        self.masked_particles = set()\n",
        "        self.invertibility_threshold = 0.01\n",
        "        self.invertibility_neighbors = 4\n",
        "\n",
        "\n",
        "        # Mean of Langevin dynamics initial distribution\n",
        "        mu_ld_noise = torch.zeros((self.particles_dim,))\n",
        "        # Standard deviation of Langevin dynamics initial distribution\n",
        "        sigma_ld_noise = torch.eye(self.particles_dim) * 0.1\n",
        "        # Langevin dynamics initial distribution\n",
        "        self.init_dist_ld = torch.distributions.MultivariateNormal(mu_ld_noise,covariance_matrix=sigma_ld_noise)\n",
        "        # Identity matrix used in the computation of the entropy (line 1)\n",
        "        self.identity_mat = torch.eye(self.particles_dim).to(device)\n",
        "        # Identity matrix used in the computation of the entropy (line 3)\n",
        "        self.identity_mat2 = torch.eye(self.num_particles).to(device)\n",
        "        # Entropy Variables, Line 1,2 and 3\n",
        "        self.logp_line1 = 0\n",
        "        self.logp_line2 = 0\n",
        "        self.logp_line3 = 0\n",
        "\n",
        "        # HMC initilization\n",
        "\n",
        "        self.X_2 = None\n",
        "        self.V_2 = None\n",
        "\n",
        "    def SVGD(self,X):\n",
        "        \"\"\"\n",
        "        Compute the Stein Variational Gradient given a set of particles\n",
        "        Inputs:\n",
        "            X: A tensor of particles\n",
        "        Outputs:\n",
        "            phi: SVGD update rule\n",
        "            phi_log_prob: SVGD update without considering the distance of the particle to itself\n",
        "            (used to calculate the log prob)\n",
        "        \"\"\"\n",
        "\n",
        "        # Make X requires grad\n",
        "        X = X.requires_grad_(True)\n",
        "        # Compute the log probability  of the particles using the target distribution\n",
        "        log_prob = self.P.log_prob(X)\n",
        "\n",
        "        # Compute the gradient of the log probability over the particles\n",
        "        self.score_func = autograd.grad(log_prob.sum(), X, retain_graph=True, create_graph=True)[0].reshape(self.num_particles, self.particles_dim)\n",
        "        # Compute the kernel matrices\n",
        "        self.K_XX, self.K_diff, self.K_h, self.K_grad, self.K_gamma = self.K.forward(X, X)\n",
        "\n",
        "        #__________INVERTIBILITY EVALUATION________\n",
        "        #store masked particles in a set\n",
        "        self.update_masked_points(self.K_XX, self.K_grad)\n",
        "\n",
        "        # - [ ] update K_grad\n",
        "\n",
        "        # - [ ] store old mask so that we don't reactivate rows taht were deactivated before.\n",
        "        curr_particles = (self.num_particles - len(self.masked_particles))\n",
        "\n",
        "        # Compute the first term of the  SVGD update\n",
        "        self.phi_term1 = self.K_XX.matmul(self.score_func) / (curr_particles) #self.num_particles\n",
        "        # Compute the second term of the  SVGD update\n",
        "        self.phi_term2 = self.K_grad.sum(0) / (curr_particles)  #self.num_particles\n",
        "        # Compute the SVGD update == h(x)\n",
        "        phi = self.phi_term1 + self.phi_term2\n",
        "\n",
        "        # Compute SVGD update without considering the distance of the particle to itself\n",
        "        #-self.identity_mat2 removes the particles itself\n",
        "        #__________ invertibility\n",
        "        phi_log_prob = (self.K_XX-self.identity_mat2).matmul(self.score_func) / (curr_particles -1)\n",
        "        phi_log_prob += (self.K_grad.sum(0) / (curr_particles -1))\n",
        "\n",
        "        for i in self.masked_particles:\n",
        "          #unset in phi and phi_log_prob\n",
        "          phi[i] = phi_log_prob[i] = torch.zeros(self.particles_dim).to(device)\n",
        "\n",
        "\n",
        "        return phi, phi_log_prob #h(x), sum(over L steps )\n",
        "\n",
        "\n",
        "\n",
        "    def compute_logprob(self, phi, X):\n",
        "        \"\"\"\n",
        "        Compute the log probability of the given particles in 3 ways\n",
        "        Inputs:\n",
        "            X: A tensor of particles\n",
        "            phi: Update rule\n",
        "        Outputs:\n",
        "            self.logp_line1: Log probability using line 1 in the paper\n",
        "            self.logp_line2: Log probability using line 2 in the paper\n",
        "        \"\"\"\n",
        "        #__________code 2____________\n",
        "        '''\n",
        "\n",
        "    ###_____\n",
        "        line4_term1 = (self.K_grad * self.score_func.unsqueeze(0)).sum(-1).sum(1)/(self.num_particles)\n",
        "        line4_term2 = -2 * self.K_gamma * (( self.K_grad.permute(1,0,2) * self.K_diff).sum(-1) - self.particles_dim * (self.K_XX - self.identity_mat2) ).sum(0)/(self.num_particles)\n",
        "        line4_term3 =  term3/(self.num_particles)\n",
        "        self.logp_line4 = self.logp_line4 - self.optim.lr * (line4_term1 + line4_term2+ line4_term3)\n",
        "\n",
        "        '''\n",
        "        # Compute the derivative of the update rule over each particle separately\n",
        "\n",
        "        grad_phi =[]\n",
        "        for i in range(len(X)):\n",
        "            grad_phi_tmp = []\n",
        "            for j in range(self.particles_dim):\n",
        "                if i in self.masked_particles or j in self.masked_particles:\n",
        "                  grad_ = torch.zeros(self.particles_dim).to(device)\n",
        "                  grad_phi_tmp.append(grad_)\n",
        "                else:\n",
        "                  grad_ = autograd.grad(phi[i][j], X, retain_graph=True)[0][i].detach()\n",
        "                  grad_phi_tmp.append(grad_)\n",
        "\n",
        "            grad_phi.append(torch.stack(grad_phi_tmp))\n",
        "\n",
        "        # self.grad_phi = torch.stack([t.to(device) for t in grad_phi])\n",
        "        self.grad_phi = torch.stack(grad_phi)\n",
        "        # Compute the log probability (Line 1) #line1\n",
        "        self.logp_line1 = self.logp_line1 - torch.log(torch.abs(torch.det(self.identity_mat + self.optim.lr * self.grad_phi)))\n",
        "        # Compute the log probability (Line 2)\n",
        "        grad_phi_trace = torch.stack( [torch.trace(grad_phi[i]) for i in range(len(grad_phi))] )\n",
        "        self.logp_line2 = self.logp_line2 - self.optim.lr * grad_phi_trace\n",
        "        # Compute the log probability (Line 2)\n",
        "        #\n",
        "        # line3_term1 = (self.K_grad * self.score_func.unsqueeze(0)).sum(-1).sum(1)/((self.num_particles - len(self.masked_particles)) -1)\n",
        "        # line3_term2 = -2 * self.K_gamma * (( self.K_grad.permute(1,0,2) * self.K_diff).sum(-1) - self.particles_dim * (self.K_XX - self.identity_mat2) ).sum(0)/((self.num_particles - len(self.masked_particles)) -1)\n",
        "        # self.logp_line3 = self.logp_line3 - self.optim.lr * (line3_term1 + line3_term2)\n",
        "\n",
        "\n",
        "    def step(self, X, V, alg):\n",
        "        \"\"\"\n",
        "        Perform one update step\n",
        "        Inputs:\n",
        "            X: A tensor of articles\n",
        "            alg: The name of the algorithm that should be used\n",
        "        Outputs:\n",
        "            X: Updated particles\n",
        "            phi_X: Update rule\n",
        "        \"\"\"\n",
        "        #_______________code 2___________\n",
        "        '''\n",
        "        X_new, violation = self.optim.step(X, phi_X)\n",
        "        ____________--> added violation  to optim\n",
        "\n",
        "        if self.with_logprob:\n",
        "            self.compute_logprob(phi_X_entropy, X, logp_term3)\n",
        "        ____________--> computed log probability\n",
        "\n",
        "        convergence = ((X_new-X)**2).sum(-1).mean()\n",
        "        svgd_converegence = ((phi_X.mean(0))**2).mean(-1)\n",
        "\n",
        "        if (alg == 'svgd') or (alg == 'ld'):\n",
        "            X = X_new.detach()\n",
        "        else:\n",
        "            X = X_new\n",
        "\n",
        "        return X, phi_X, violation, convergence, num_componenets, norm_inv_values, svgd_converegence\n",
        "\n",
        "        '''\n",
        "        if alg == 'svgd':\n",
        "            # Compute the SVGD update\n",
        "            phi_X, phi_X_log_prob = self.SVGD(X)\n",
        "        # elif alg == 'ld':\n",
        "        #     # Compute the Langevin Dynamics update\n",
        "        #     phi_X = self.LD(X, with_noise=True)\n",
        "        #     phi_X_log_prob = phi_X\n",
        "        # elif alg == 'dld':\n",
        "        #     # Compute the Langevin Dynamics update\n",
        "        #     phi_X = self.LD(X, with_noise=False)\n",
        "        #     phi_X_log_prob = phi_X\n",
        "        # elif alg == 'hmc':\n",
        "        #     # Compute the Hamiltionian Monte-carlo update\n",
        "        #     phi_X = self.HMC(X, V)\n",
        "        #     phi_X_log_prob = phi_X\n",
        "\n",
        "        # Update the particles using the computed update\n",
        "        X_new = self.optim.step(X, phi_X)\n",
        "        self.phi_X = phi_X\n",
        "        # Compute the log probability for the updated particles\n",
        "        self.compute_logprob(phi_X_log_prob, X)\n",
        "        # keep track of convergence for stein\n",
        "\n",
        "        convergence = ((X_new-X)**2).sum(-1).mean()\n",
        "        svgd_converegence = ((phi_X.mean(0))**2).mean(-1)\n",
        "        # Detach the particles to prepare for the next iteration\n",
        "        if alg in ['svgd', 'ld']:\n",
        "            X = X_new.detach()\n",
        "        else:\n",
        "            X = X_new\n",
        "\n",
        "        return X, phi_X, convergence, svgd_converegence\n",
        "\n",
        "    def update_masked_points(self, distances):\n",
        "        \"\"\"\n",
        "        check if there is a new masked point\n",
        "        Inputs:\n",
        "            X: distance matrix of particles\n",
        "        \"\"\"\n",
        "        mask = (distances > self.invertibility_threshold).int()\n",
        "        active_neighbors = (mask.sum(-1) > self.invertibility_neighbors).int()\n",
        "\n",
        "        #modify K_XX and K_grad\n",
        "        # distances = active_neighbors.unsqueeze(-1) * distances\n",
        "        # grad = active_neighbors.unsqueeze(-1) * grad\n",
        "\n",
        "        new_mask = torch.where(active_neighbors == False)[0]\n",
        "\n",
        "        for i in new_mask:\n",
        "          self.masked_particles.add(i.item())\n",
        "          #dynamically update the number of currently active particles\n",
        "\n",
        "\n",
        "##\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwVMAObJ2XFp"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dudEH5vAe4Fn"
      },
      "outputs": [],
      "source": [
        "def my_experiment(dim, n_gmm, num_particles, num_steps, kernel_sigma, gmm_std, lr, mu_init, sigma_init, sigma_V_init, dist_type, run_alg):\n",
        "    \"\"\"\n",
        "    Perform one experiment of running SVGD and computing the entropy\n",
        "    Inputs:\n",
        "        dim: Number of dimensions of the particles\n",
        "        n_gmm: Number of mixutres of the target GMM distribution\n",
        "        num_particles: Number of particles\n",
        "        num_step: Number of update steps\n",
        "        kernel_sigma: Kernel standard diviation\n",
        "        gmm_std: The target distribution standard diviation\n",
        "        lr: Learning rate\n",
        "        sigma_init: Standard deviation of the initial distribution\n",
        "        mu_init: Mean of the initial distribution\n",
        "        dist: Type of the target distribution\n",
        "    Outputs:\n",
        "        sampler_entr_svgd: The SVGD Entropy\n",
        "        sampler_entr_ld: The LD Entropy\n",
        "        gt_entr: The ground truth Entropy\n",
        "        charts: The list of density plots with particles.(a plot for each svgd step)\n",
        "    \"\"\"\n",
        "\n",
        "    # Creating the initial distribution\n",
        "    mu = torch.full((dim,), mu_init).to(device).to(torch.float32)\n",
        "    sigma = (torch.eye(dim) * sigma_init).to(device)\n",
        "    init_dist = torch.distributions.MultivariateNormal(mu,covariance_matrix=sigma)\n",
        "\n",
        "    # Sampling a tensor of particles from the initial distribution\n",
        "    X_init = init_dist.sample((num_particles,)).to(device)\n",
        "\n",
        "    init_V_dist = torch.distributions.MultivariateNormal(torch.zeros(dim).to(device),covariance_matrix = (torch.eye(dim) * sigma_V_init).to(device))\n",
        "    V_init = init_V_dist.sample((num_particles,)).to(device)\n",
        "\n",
        "    if dist_type == 'gmm':\n",
        "        # Figures 4b, high dimensionality figures\n",
        "        dist = GMMDist(dim=dim, n_gmm=n_gmm, sigma=gmm_std)\n",
        "    elif dist_type == 'gauss':\n",
        "        # Figure 4a\n",
        "        dist = torch.distributions.MultivariateNormal(torch.Tensor([-0.6871,0.8010]).to(device),covariance_matrix= 5 *torch.Tensor([[0.2260,0.1652],[0.1652,0.6779]]).to(device))\n",
        "\n",
        "    # Create an entropy experiment object\n",
        "    experiment = Entropy(dist, RBF(kernel_sigma, num_particles), Optim(lr), num_particles=num_particles, particles_dim=dim)\n",
        "\n",
        "    # Generate plots only if dim == 2\n",
        "    if dim == 2:\n",
        "        gauss_chart = get_density_chart(dist, d=7.0, step=0.1, dist_type=dist_type)\n",
        "        init_chart = gauss_chart + get_particles_chart(X_init.cpu().numpy())\n",
        "    else:\n",
        "        init_chart = None\n",
        "\n",
        "    # Compute the groud truth entropy\n",
        "    gt_entr = - dist.log_prob(dist.sample((500,))).mean().cpu().item()\n",
        "\n",
        "    def main_loop(alg, X, V, steps):\n",
        "        \"\"\"\n",
        "        Perform SVGD updates\n",
        "        Inputs:\n",
        "            X: A tensor of particles\n",
        "            steps: The number of SVGD steps\n",
        "        Outputs:\n",
        "            sampler_entr_svgd: The SVGD Entropy\n",
        "            gt_entr: The ground truth Entropy\n",
        "            charts: The list of density plots with particles.(a plot for each svgd step)\n",
        "        \"\"\"\n",
        "        # gamma = 1.0 / (1e-8 + 2 * sigma**2)\n",
        "        active_particles = []\n",
        "        entropies = []\n",
        "        convergence_values = []\n",
        "        charts = []\n",
        "        X_svgd_=[]\n",
        "        for t in range(steps):\n",
        "            # Perform one update step\n",
        "            X, phi_X, conv, svgd_conv= experiment.step(X, V, alg)\n",
        "            # Save the intermidiate position of the particles\n",
        "            X_svgd_.append(X.clone())\n",
        "            # Generate plots only if dim == 2\n",
        "            entropy_l_1 =  -(init_dist.log_prob(X_init) + experiment.logp_line1).mean().item()\n",
        "            entropy_l_2 =  -(init_dist.log_prob(X_init) + experiment.logp_line2).mean().item()\n",
        "\n",
        "\n",
        "            # how many neighbors per particle for the current sigma\n",
        "            # neighbor_avg = (((experiment.K_XX > 0.01).int()).sum(-1)).float().mean()\n",
        "            # active_particles.append(active)\n",
        "            # sigma_to_neigh[t] = (curr_sig, neighbor_avg.item())\n",
        "\n",
        "            if dim == 2:\n",
        "                masked = X.detach().cpu().numpy()[list(experiment.masked_particles),:]\n",
        "                chart_ = gauss_chart + get_particles_chart(X.detach().cpu().numpy(), masked)\n",
        "                charts.append(chart_)\n",
        "\n",
        "            convergence_values.append(svgd_conv)\n",
        "            entropies.append([entropy_l_1, entropy_l_2])\n",
        "\n",
        "        X_svgd_ = torch.stack(X_svgd_)\n",
        "        convergence_values = torch.stack(convergence_values).cpu().detach().numpy()\n",
        "\n",
        "        # Compute the entropy of the particles using the log probability\n",
        "        sampler_entr =  -(init_dist.log_prob(X_init) + experiment.logp_line2).mean().item()\n",
        "\n",
        "        print('################# sampler_entr: ', alg, sampler_entr)\n",
        "        # print('################# diff X_t X_t-1: ', torch.norm(experiment.phi_X).detach().cpu().numpy())\n",
        "\n",
        "        # Print line 1 and line 2\n",
        "        # print(t, ' entropy svgd (line 1): ',  -(init_dist.log_prob(X_init) + experiment.logp_line1).mean().item())\n",
        "        # print(t, ' entropy svgd (line 2): ',  -(init_dist.log_prob(X_init) + experiment.logp_line2).mean().item())\n",
        "\n",
        "        return sampler_entr, charts, convergence_values , entropies\n",
        "\n",
        "    # Print the ground truth entropy\n",
        "    # print('entropy gt (logp): ', gt_entr)\n",
        "    # print()\n",
        "\n",
        "    # Run SVGD\n",
        "    if run_alg['svgd']:\n",
        "        # print('___________SVGD___________')\n",
        "        sampler_entr_svgd, charts_svgd, conv, ent= main_loop('svgd', X_init.clone().detach(), None, steps=num_steps)\n",
        "    else:\n",
        "        sampler_entr_svgd, charts_svgd, conv, ent  = None, None, None, None\n",
        "\n",
        "    # # Run Lengevin Dynamics\n",
        "    if run_alg['ld']:\n",
        "        # print('___________LD___________')\n",
        "        sampler_entr_ld, charts_ld = main_loop('ld', X_init.clone().detach(), None, steps=num_steps[1])\n",
        "    else:\n",
        "        sampler_entr_ld, charts_ld = None, None\n",
        "\n",
        "    # Run Deterministic Lengevin Dynamics\n",
        "    if run_alg['dld']:\n",
        "        # print('___________DLD___________')\n",
        "        sampler_entr_dld, charts_dld = main_loop('dld', X_init.clone().detach(), None, steps=num_steps[1])\n",
        "    else:\n",
        "        sampler_entr_dld, charts_dld = None, None\n",
        "\n",
        "    # Run HMC\n",
        "    if run_alg['hmc']:\n",
        "        # print('___________HMC___________')\n",
        "        sampler_entr_hmc, charts_hmc = main_loop('hmc', X_init.clone().detach(), V_init.clone(), steps=num_steps[2])\n",
        "    else:\n",
        "        sampler_entr_hmc, charts_hmc = None, None\n",
        "\n",
        "\n",
        "    return sampler_entr_svgd, charts_svgd, conv, ent, gt_entr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1eW9q1TNb_I"
      },
      "source": [
        "# Reproduce Paper Figures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ5BYHOqNb_I"
      },
      "source": [
        "### Figure 4a"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Weather to use the GMM class or not (Figure 4a: gmm == False)\n",
        "dist_type = 'gauss'\n",
        "# Dimension of the particles\n",
        "dim = 2\n",
        "# Learning Rate\n",
        "lr = 0.5\n",
        "# Number of mixtures\n",
        "n_gmm = 1\n",
        "# Target distribution's standard deviation, means are computed automatically\n",
        "gmm_std = 1\n",
        "# Number of particles\n",
        "num_particles = 400\n",
        "# Number of update steps\n",
        "num_steps = 1000\n",
        "# Kernel variance\n",
        "kernel_variance = 'forth'\n",
        "# Standard deviation of the initial distribution HMC\n",
        "sigma_V_init = 5\n",
        "# Dictionary used to select whare algorithms to run\n",
        "run_alg = {'svgd':True, 'ld':False, 'dld':False, 'hmc':False}\n",
        "\n",
        "\n",
        "\n",
        "# Mean of the initial distribution\n",
        "mu_init = 0\n",
        "# Standard deviation of the initial distribution\n",
        "sigma_init = 6\n",
        "# Do one experiment for initial distribution N(0, 6)\n",
        "# sensitivity_to_para = c.defaultdict(list)\n",
        "# for kernel_variance in kernel_variances:\n",
        "#   for lr in lrs:\n",
        "sampler_entr_svgd, charts_svgd, conv, ent, gt_entr = my_experiment(dim, n_gmm, num_particles, num_steps, kernel_variance, gmm_std, lr, mu_init, sigma_init, sigma_V_init, dist_type, run_alg)\n",
        "    # sensitivity_to_para[(lr, kernel_variance)] = [gt_entr, ent, conv]\n",
        "\n",
        "# sampler_entr_svgd, sampler_entr_ld, sampler_entr_hmc, sampler_entr_dld, gt_entr, charts_svgd= my_experiment(dim, n_gmm, num_particles, num_steps, kernel_variance, gmm_std, lr, mu_init, sigma_init, sigma_V_init, dist_type, run_alg)\n",
        "# print('##############################')\n",
        "\n",
        "#________________________\n",
        "\n",
        "# lr = 0.5\n",
        "# # Number of update steps\n",
        "# num_steps = [200, 20, 50]\n",
        "# # Mean of the initial distribution\n",
        "# mu_init = 4\n",
        "# # Standard deviation of the initial distribution\n",
        "# sigma_init = 0.2\n",
        "# # Do one experiment for initial distribution N(4, 0.2)\n",
        "# init_chart2, sampler_entr_svgd2, sampler_entr_ld2, sampler_entr_hmc2, sampler_entr_dld2, gt_entr2, charts_svgd2, charts_ld2, charts_hmc2, charts_dld2 = my_experiment(dim, n_gmm, num_particles, num_steps, kernel_variance, gmm_std, lr, mu_init, sigma_init, sigma_V_init, dist_type, run_alg)\n",
        "\n",
        "# # Concatenate multiple plots together\n",
        "# alt.vconcat(init_chart | charts_svgd[-1] | charts_dld[-1] | charts_ld[-1] | charts_hmc[-1], init_chart2 | charts_svgd2[-1] | charts_dld2[-1] | charts_ld2[-1] | charts_hmc2[-1])\n",
        "\n",
        "\n",
        "\"\"\"-> run experiment for\n",
        "\n",
        "- $\\sigma_{kernel}$ in `[0.1, 1, 3, 5, 7, 10, 100]`\n",
        "- for $n_{particles}$ in `[200, 100, 20, 10]`\n",
        "# \"\"\"\n",
        "\n",
        "# #set the variables that remain unchanged\n",
        "# dist_type = 'gauss'\n",
        "# dim = 2\n",
        "# lr = 0.5\n",
        "# n_gmm = 1\n",
        "# gmm_std = 1\n",
        "# num_particles = [200, 100, 20, 10]\n",
        "# num_steps = [100, 20, 15]\n",
        "# kernel_variance = [0.1, 1, 3, 5, 7, 10, 100]\n",
        "# sigma_V_init = 5\n",
        "# run_alg = {'svgd':True, 'ld':False, 'dld':False, 'hmc':False}\n",
        "\n",
        "\n",
        "# mu_init = 0\n",
        "# sigma_init = 6\n",
        "\n",
        "\n",
        "\n",
        "# sampler_entr_svgd, charts_svgd, conv, ent, gt_entr, sigmas = my_experiment(dim, n_gmm, num_particles, num_steps, kernel_variance, gmm_std, lr, mu_init, sigma_init, sigma_V_init, dist_type, run_alg)\n",
        "\n",
        "\n",
        "\n",
        "# particles_variance_map\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# for n in num_particles:\n",
        "#     entropies = [particles_variance_map[(n, v)] for v in kernel_variance]\n",
        "#     plt.scatter(kernel_variance, entropies, label=f'{n} particles')\n",
        "\n",
        "# plt.xlabel('Kernel variance σ')\n",
        "# plt.ylabel('Entropy H(q^L)')\n",
        "# plt.legend()\n",
        "# plt.title('Effect of σ on H(q^L) for SVGD')\n",
        "# plt.xscale('log')\n",
        "# plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "au6zpC_F-myz",
        "outputId": "2217d2cb-1f97-4556-aa71-bfe2ecabbe6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Entropy.update_masked_points() takes 2 positional arguments but 3 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7dba172cf53f>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# for kernel_variance in kernel_variances:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#   for lr in lrs:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0msampler_entr_svgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharts_svgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_entr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_particles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_variance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmm_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_V_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_alg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;31m# sensitivity_to_para[(lr, kernel_variance)] = [gt_entr, ent, conv]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-2f9758ace608>\u001b[0m in \u001b[0;36mmy_experiment\u001b[0;34m(dim, n_gmm, num_particles, num_steps, kernel_sigma, gmm_std, lr, mu_init, sigma_init, sigma_V_init, dist_type, run_alg)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_alg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'svgd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# print('___________SVGD___________')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0msampler_entr_svgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharts_svgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'svgd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_init\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0msampler_entr_svgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharts_svgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-2f9758ace608>\u001b[0m in \u001b[0;36mmain_loop\u001b[0;34m(alg, X, V, steps)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# Perform one update step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvgd_conv\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0;31m# Save the intermidiate position of the particles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mX_svgd_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-fb0c8cf6f68b>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, X, V, alg)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0malg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svgd'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;31m# Compute the SVGD update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mphi_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi_X_log_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0;31m# elif alg == 'ld':\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m#     # Compute the Langevin Dynamics update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-fb0c8cf6f68b>\u001b[0m in \u001b[0;36mSVGD\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m#__________INVERTIBILITY EVALUATION________\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m#store masked particles in a set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_masked_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK_XX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# - [ ] update K_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Entropy.update_masked_points() takes 2 positional arguments but 3 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluating the number of lost particles across svgd steps"
      ],
      "metadata": {
        "id": "4anIjDiEN5cN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "particles_lost = pd.DataFrame({'svgd_step': np.arange(num_steps), 'particles': act})\n",
        "plt.plot(particles_lost['svgd_step'], particles_lost['particles'], color= '#00008B')\n",
        "plt.xlabel('svgd_steps')\n",
        "plt.ylabel('current number of active particles')\n",
        "plt.title('Evaluating particle loss according to neighborhood kernel threshold')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "5sW3jegXN4-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qualitative Convergence"
      ],
      "metadata": {
        "id": "f7WBBBpNxSRl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OflzLlj3Nb_J"
      },
      "outputs": [],
      "source": [
        "charts_svgd[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "charts = charts_svgd"
      ],
      "metadata": {
        "id": "Mtl6fIDQx2TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qualitative_conv = charts[0].properties(title='init step')\n",
        "for i in range(25, num_steps, 25):\n",
        "    qualitative_conv |= charts[i].properties(title=f'step {i}')\n",
        "qualitative_conv |= charts[-1].properties(title=f'step {num_steps}')\n",
        "qualitative_conv.save('./' + '.html') #add chart directory\n"
      ],
      "metadata": {
        "id": "lLu-EmFlxR-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qualitative_conv"
      ],
      "metadata": {
        "id": "1fKcEcYQyEhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=DC143C>Convergence is qualitatively verified!</font>"
      ],
      "metadata": {
        "id": "J4dUkR6vyTCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantitative Convergence\n",
        "\n",
        "\n",
        "\n",
        "*   Tracking Stein Idendity convergence\n",
        "\n",
        "*   Tracking Entropy Convergence\n",
        "\n"
      ],
      "metadata": {
        "id": "BJjXdVfdyrLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gt_entr"
      ],
      "metadata": {
        "id": "Btj-vpwc4-F7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entropies[]"
      ],
      "metadata": {
        "id": "cdNvWSNq5BNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stein_id_0 = pd.DataFrame({'svgd_step': np.arange(num_steps), 'stein identity': conv})\n",
        "plt.plot(stein_id_0['svgd_step'], stein_id_0['stein identity'], color= '#0000FF')\n",
        "plt.xlabel('svgd_steps')\n",
        "plt.ylabel('stein identity')\n",
        "plt.title('Verifying convergence with Stein Identity')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "RBYK05MyxbsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ent1 = [ent[i][0] for i in range(len(ent))]\n",
        "ent2 = [ent[i][1] for i in range(len(ent))]\n",
        "\n",
        "df = pd.DataFrame({'svgd_step':np.arange(num_steps), 'line1':ent1, 'line2':ent2})\n",
        "## gt entropy\n",
        "plt.axhline(y = gt_entr, color='r', linestyle='-', label = 'gt_entropy')\n",
        "# line 1\n",
        "plt.plot(df['svgd_step'], df['line1'], label = 'line 1', color = '#006400')\n",
        "#line 2\n",
        "plt.plot(df['svgd_step'], df['line2'], label = 'line 2', color = '#00CED1')\n",
        "# add extra axis for the stein id\n",
        "# plt.plot(stein_id_0['svgd_step'], stein_id_0['stein identity'], color= '#0000FF')\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel('itr')\n",
        "plt.ylabel('entropies')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P3kqbmcApZZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Average number of Neighbors for particles when σ is set to 'forth'\n",
        "\n",
        "\n",
        "\n",
        "> We track the number of neighbors in order <font color=FF7F50>evaluate the sensitivity of convergence to the σ at every step</font>\n",
        "\n"
      ],
      "metadata": {
        "id": "IaCBwztmzITY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=FF7F50>Every 10 steps observe the corresponding kernel $σ$ and the average number of particle neighbors</font>"
      ],
      "metadata": {
        "id": "yvB1UVue3vPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sigFORTH_neighbors = [sig[l][1] for l in range(0, num_steps, 10)]\n",
        "sigFORTH_sigmas = [sig[l][0] for l in range(0, num_steps, 10)]"
      ],
      "metadata": {
        "id": "QktZLzRY3ISp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sigMEAN_neighbors = [sig[l][1] for l in range(0, num_steps, 10)]\n",
        "sigMEAN_sigmas = [sig[l][0] for l in range(0, num_steps, 10)]"
      ],
      "metadata": {
        "id": "jgPHARqH4DN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sigMEDIAN_neighbors = [sig[l][1] for l in range(0, num_steps, 10)]\n",
        "sigMEDIAN_sigmas = [sig[l][0] for l in range(0, num_steps, 10)]"
      ],
      "metadata": {
        "id": "-MV2876h4C9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sig5_neighbors = [sig[l][1] for l in range(0, num_steps, 10)]\n",
        "sig5_sigmas = [sig[l][0] for l in range(0, num_steps, 10)]"
      ],
      "metadata": {
        "id": "WeMRn-Ms4MYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=FF7F50>Storing number of active particles for every kernel</font>"
      ],
      "metadata": {
        "id": "v6QBU2UDZw2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "active_particles_forth = act"
      ],
      "metadata": {
        "id": "VJCzBOLkaBl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "active_particles_mean = act"
      ],
      "metadata": {
        "id": "RyX6bh0paPM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "active_particles_median = act"
      ],
      "metadata": {
        "id": "0A2pSbBQaOiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(active_particles_median[::10])"
      ],
      "metadata": {
        "id": "5ljVFiePBij3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#barring\n",
        "a1 = active_particles_median[::10]\n",
        "a2 = active_particles_forth[::10]\n",
        "a3 = active_particles_mean[::10]\n",
        "x = np.arange(len(a1))\n",
        "width = 0.2\n",
        "\n",
        "# Creating the plot\n",
        "fig, ax = plt.subplots()\n",
        "r1 = ax.bar(x - 1.5*width, a2, width, label='FORTH', color = '#8A2BE2')\n",
        "r2 = ax.bar(x - 0.5*width, a3, width, label='MEAN', color = '#7FFF00')\n",
        "r3 = ax.bar(x + 0.5*width, a1, width, label='MEDIAN', color = '#FF69B4')\n",
        "\n",
        "\n",
        "# Add some text for labels, title, and custom x-axis tick labels\n",
        "ax.set_xlabel('every 10 steps')\n",
        "ax.set_ylabel('active particles per kernel variance')\n",
        "ax.set_title('neighborhood population through svgd updates per kernel variance')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([str(i) for i in range(1, 21)])\n",
        "ax.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tzO0sKY4By9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#barring\n",
        "a1 = active_particles_median[::10]\n",
        "a2 = active_particles_median[::10]\n",
        "a3 = active_particles_median[::10]\n",
        "x = np.arange(len(a))\n",
        "width = 0.2\n",
        "\n",
        "# Creating the plot\n",
        "fig, ax = plt.subplots()\n",
        "r1 = ax.bar(x - 1.5*width, sigFORTH_neighbors, width, label='FORTH')\n",
        "r2 = ax.bar(x - 0.5*width, sigMEAN_neighbors, width, label='MEAN')\n",
        "r3 = ax.bar(x + 0.5*width, sigMEDIAN_neighbors, width, label='MEDIAN')\n",
        "r4 = ax.bar(x + 1.5*width, sig5_neighbors, width, label='5')\n",
        "\n",
        "# Add some text for labels, title, and custom x-axis tick labels\n",
        "ax.set_xlabel('every 10 steps')\n",
        "ax.set_ylabel('average neighborhood populace')\n",
        "ax.set_title('neighborhood population through svgd updates per kernel variance')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels([str(i) for i in range(1, 21)])\n",
        "ax.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rWZdln3Fjr4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extracting data\n",
        "steps = list(sig.keys())\n",
        "sigmas = [v[0] for v in sig.values()]\n",
        "avg_neighbors = [v[1] for v in sig.values()]\n",
        "\n",
        "# Creating the plot\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "# Plotting sigma values\n",
        "color = 'tab:red'\n",
        "ax1.set_xlabel('SVGD Step')\n",
        "ax1.set_ylabel('Sigma (Kernel Variance)', color=color)\n",
        "ax1.plot(steps, sigmas, color=color)\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "# Instantiating a second y-axis for average number of neighbors\n",
        "ax2 = ax1.twinx()\n",
        "color = 'tab:blue'\n",
        "ax2.set_ylabel('Average Number of Neighbors', color=color)\n",
        "ax2.plot(steps, avg_neighbors, color=color)\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "# Showing the plot\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZeO5TJmwzllk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5G4OQuqjyrHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Quantity of neighbors per particle for a kernel variance with invertibility 'proven' (σ = 5)\n",
        "\n"
      ],
      "metadata": {
        "id": "2wh44VGeA46O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sig"
      ],
      "metadata": {
        "id": "l5mAgar-Ab10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxKZrj_uNb_I"
      },
      "outputs": [],
      "source": [
        "# Weather to use the GMM class or not (Figure 4a: gmm == False)\n",
        "dist_type = 'gauss'\n",
        "# Dimension of the particles\n",
        "dim = 2\n",
        "# Learning Rate\n",
        "lr = 0.5\n",
        "# Number of mixtures\n",
        "n_gmm = 1\n",
        "# Target distribution's standard deviation, means are computed automatically\n",
        "gmm_std = 1\n",
        "# Number of particles\n",
        "num_particles = 200\n",
        "# Number of update steps\n",
        "num_steps = 200\n",
        "# Kernel variance\n",
        "kernel_variance = ['mean']\n",
        "# Standard deviation of the initial distribution HMC\n",
        "sigma_V_init = 5\n",
        "# Dictionary used to select whare algorithms to run\n",
        "run_alg = {'svgd':True, 'ld':False, 'dld':False, 'hmc':False}\n",
        "\n",
        "\n",
        "\n",
        "# Mean of the initial distribution\n",
        "mu_init = 0\n",
        "# Standard deviation of the initial distribution\n",
        "sigma_init = 6\n",
        "# Do one experiment for initial distribution N(0, 6)\n",
        "# sensitivity_to_para = c.defaultdict(list)\n",
        "# for kernel_variance in kernel_variances:\n",
        "#   for lr in lrs:\n",
        "sampler_entr_svgd, charts_svgd, conv, ent, gt_entr, sigmas = my_experiment(dim, n_gmm, num_particles, num_steps, kernel_variance, gmm_std, lr, mu_init, sigma_init, sigma_V_init, dist_type, run_alg)\n",
        "    # sensitivity_to_para[(lr, kernel_variance)] = [gt_entr, ent, conv]\n",
        "\n",
        "# sampler_entr_svgd, sampler_entr_ld, sampler_entr_hmc, sampler_entr_dld, gt_entr, charts_svgd= my_experiment(dim, n_gmm, num_particles, num_steps, kernel_variance, gmm_std, lr, mu_init, sigma_init, sigma_V_init, dist_type, run_alg)\n",
        "# print('##############################')\n",
        "\n",
        "#________________________\n",
        "\n",
        "# lr = 0.5\n",
        "# # Number of update steps\n",
        "# num_steps = [200, 20, 50]\n",
        "# # Mean of the initial distribution\n",
        "# mu_init = 4\n",
        "# # Standard deviation of the initial distribution\n",
        "# sigma_init = 0.2\n",
        "# # Do one experiment for initial distribution N(4, 0.2)\n",
        "# init_chart2, sampler_entr_svgd2, sampler_entr_ld2, sampler_entr_hmc2, sampler_entr_dld2, gt_entr2, charts_svgd2, charts_ld2, charts_hmc2, charts_dld2 = my_experiment(dim, n_gmm, num_particles, num_steps, kernel_variance, gmm_std, lr, mu_init, sigma_init, sigma_V_init, dist_type, run_alg)\n",
        "\n",
        "# # Concatenate multiple plots together\n",
        "# alt.vconcat(init_chart | charts_svgd[-1] | charts_dld[-1] | charts_ld[-1] | charts_hmc[-1], init_chart2 | charts_svgd2[-1] | charts_dld2[-1] | charts_ld2[-1] | charts_hmc2[-1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "WZfTpW-PSviZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pff = sensitivity_to_para[(0.1, 'median')][-1]"
      ],
      "metadata": {
        "id": "vrxukcXGR8Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyyL2O4CNb_K"
      },
      "source": [
        "-> run experiment for\n",
        "\n",
        "- $\\sigma_{kernel}$ in `[0.1, 1, 3, 5, 7, 10, 100]`\n",
        "- for $n_{particles}$ in `[200, 100, 20, 10]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AckTgUjNb_K"
      },
      "outputs": [],
      "source": [
        "#set the variables that remain unchanged\n",
        "dist_type = 'gauss'\n",
        "dim = 2\n",
        "lr = 0.5\n",
        "n_gmm = 1\n",
        "gmm_std = 1\n",
        "num_particles = [200, 100, 20, 10]\n",
        "num_steps = [100, 20, 15]\n",
        "kernel_variance = [0.1, 1, 3, 5, 7, 10, 100]\n",
        "sigma_V_init = 5\n",
        "run_alg = {'svgd':True, 'ld':False, 'dld':False, 'hmc':False}\n",
        "\n",
        "\n",
        "mu_init = 0\n",
        "sigma_init = 6\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avBXTU0GNb_K"
      },
      "outputs": [],
      "source": [
        "particles_variance_map = {(n, v): 0 for n in num_particles for v in kernel_variance}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nz3PFhxjNb_K"
      },
      "outputs": [],
      "source": [
        "init_chart, sampler_entr_svgd, charts_svgd = my_experiment(dim, n_gmm, n, num_steps, v, gmm_std, lr, mu_init, sigma_init, sigma_V_init, dist_type, run_alg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS0DuL4-Nb_K"
      },
      "outputs": [],
      "source": [
        "for pv in particles_variance_map:\n",
        "    n, v = pv\n",
        "    svgd = my_experiment(dim, n_gmm, n, num_steps, v, gmm_std, lr, mu_init, sigma_init, sigma_V_init, dist_type, run_alg)\n",
        "    particles_variance_map[pv] = svgd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxktq3a7Nb_K"
      },
      "outputs": [],
      "source": [
        "particles_variance_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt3zRoa6Nb_L"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEBBOu2FNb_L"
      },
      "outputs": [],
      "source": [
        "for n in num_particles:\n",
        "    entropies = [particles_variance_map[(n, v)] for v in kernel_variance]\n",
        "    plt.scatter(kernel_variance, entropies, label=f'{n} particles')\n",
        "\n",
        "plt.xlabel('Kernel variance σ')\n",
        "plt.ylabel('Entropy H(q^L)')\n",
        "plt.legend()\n",
        "plt.title('Effect of σ on H(q^L) for SVGD')\n",
        "plt.xscale('log')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bts4EIIuNb_T"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "8360f93b682846d9db2e24bd21c06b219fd218969b51c91bbf4fd90fc42d351f"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}