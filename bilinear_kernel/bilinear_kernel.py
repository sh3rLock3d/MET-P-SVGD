# -*- coding: utf-8 -*-
"""Copy of ToyExperiments_version0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VgU5P-KTY2jZSAXWZ0CyT_wBQao-49KN

# Imports
"""

import torch
import numpy as np
from torch import autograd
import altair as alt
alt.data_transformers.enable('default', max_rows=None)
import pandas as pd
import math
import pickle
from tqdm import tqdm
import matplotlib.pyplot as plt

"""# Global Variables"""

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print('device:',device)

"""# Helper Functions"""

def get_density_chart(P, d=7.0, step=0.1, dist_type=None):
    """
    Given a probability distribution, return a density chart (Heatmap)
    Inputs:
        P: Probability distribution
        d: A value used to bound the meshgrid
        step: A value used in the arange method to create the meshgrid
    Outputs:
        chart: Altair object corresponding to a density plot
    """
    xv, yv = torch.meshgrid([torch.arange(-d, d, step), torch.arange(-d, d, step)])
    pos_xy = torch.cat((xv.unsqueeze(-1), yv.unsqueeze(-1)), dim=-1)
    p_xy = P.log_prob(pos_xy.to(device)).exp().unsqueeze(-1).cpu()


    df = torch.cat([pos_xy, p_xy], dim=-1).numpy()
    df = pd.DataFrame({
        'x': df[:, :, 0].ravel(),
        'y': df[:, :, 1].ravel(),
        'p': df[:, :, 2].ravel(),})

    chart = alt.Chart(df).mark_point().encode(

        x=alt.X('x:Q', axis=alt.Axis(title='', labelFontSize=20, tickSize=12)),
        y=alt.Y('y:Q', axis=alt.Axis(title='', labelFontSize=20, tickSize=12)),
        color=alt.Color('p:Q', scale=alt.Scale(scheme='lightorange')),
        tooltip=['x','y','p']).properties(
        width=220,
        height=190
    )


    return chart



def get_particles_chart(X, X_svgd=None):
    """
    Given a set of points, return a scatter plot
    Inputs:
        X: Final positions of the particles after applying svgd
        X_svgd: Intermidiate position of the particles while applying svgd. If None do not add them to the plot
    Outputs:
        chart: Altair object corresponding to a scatter plot
    """
    df = pd.DataFrame({
        'x': X[:, 0],
        'y': X[:, 1],})

    chart = alt.Chart(df).mark_circle(color='black').encode(x='x:Q',y='y:Q')

    if X_svgd is not None:
        for i in range(np.shape(X_svgd)[1]):
            df_trajectory = pd.DataFrame({'x': X_svgd[:,i,0],'y': X_svgd[:,i,1],})
            chart += alt.Chart(df_trajectory).mark_line().mark_circle(color='green').encode(x='x:Q',y='y:Q')

    return chart


def filter(results, constraints):
    """
    Given a dictionary of experiments results, search it given a list of constraints
    Inputs:
        results: A dictionary of experimental results
        constraints: A list of configuration constraints
    Outputs:
        Elements based on the input configurations of experiments
    """
    configs = []
    for i in range(len(results['configs'])):
        check_bool = True
        for j in range(len(results['configs'][i])):
            if constraints[j] == '*':
                continue
            if results['configs'][i][j] != constraints[j]:
                check_bool = False
                break
        if check_bool:
            configs.append(i)

    return np.array(results['sampler_entr_svgd'])[configs].tolist(), np.array(results['gt_entr'])[configs].tolist(), np.array(results['charts_svgd'])[configs].tolist(), np.array(results['init_chart'])[configs].tolist()

"""# Kernel"""


class Bilinear_kernel:
    """
    Bilinear kernel. K(x,y)= 1 + (xy^t)/C where C is norm and a constant and is equal to maximum 2*(norm2 of particle)^2 for all particles
    Inputs:
        particles_dim: Dimention of particles
        num_particles: Number of particles
    """    
    def __init__(self, particles_dim, num_particles):
        self.num_particles = num_particles
        self.particles_dim = particles_dim
        self.I = torch.eye(self.num_particles,self.num_particles).to(device)
        self.I2 = torch.ones(self.num_particles).unsqueeze(-1).unsqueeze(-1).to(device)
        self.I_phi = torch.eye(self.num_particles,self.num_particles).unsqueeze(-1).to(device)

    def forward(self, input_1, input_2):
        """
        Given two tensors of particles, return the matrix of distances between each particle
        Inputs:
            input_1: Particles coordinates
            input_2: Particles coordinates
        Outputs:
            kappa: Bilinear matrix
            grad: derivation of Bilinear kernel
            kappa_phi: Bilinear matrix without considering the distance of the particle to itself
            grad_ph: derivation of Bilinear kernel without considering the distance of the particle to itself
        """        
        norm = 2 * torch.max((input_1**2).sum(-1)).detach() # 2 * C^2
        kappa = (torch.matmul(input_1, (input_2).t())/norm)+1        
        grad = (input_2) / (self.I2 * norm)
        indices = np.arange(0,self.num_particles)
        grad[indices, indices] = 2 * grad[indices, indices]        
        kappa_phi = kappa -  kappa * self.I 
        grad_phi = grad -  grad * self.I_phi
        
        return kappa, grad, kappa_phi, grad_phi

"""# GMM"""

class GMMDist(object):
    """
    Gaussian Mixture Model
    Inputs:
        dim: Number of dimensions of the random variable
        n_gmm: Number of mixtures
        sigma: Mixutures' standard diviation
    """
    def __init__(self, dim, n_gmm, sigma):
        def _compute_mu(i):
            """
            Compute the mean of each mixture
            Inputs:
                i: Index of the mixture
            Outputs:
                The mean of each dimension of the random variable
            """
            return 4 * torch.Tensor([[torch.tensor(i * math.pi / (n_gmm/2)).sin(),torch.tensor(i * math.pi / (n_gmm/2)).cos()] + [torch.rand((1,)) * 2 - 1 for _ in range(dim - 2)]])

        # Generate a list of equal probabilities between the mixtures
        self.mix_probs = (1.0/n_gmm) * torch.ones(n_gmm).to(device)
        # Compute the means
        self.means = torch.cat([_compute_mu(i) for i in range(n_gmm)], dim=0).to(device)
        # Compute the standard deviations
        self.sigma = sigma
        self.std = torch.stack([torch.ones(dim).to(device) * self.sigma for i in range(len(self.mix_probs))], dim=0)


    def sample(self, n):
        """
        Sample a tensor of particles
        Inputs:
            n: Number of particles
        Outputs:
            A tensor of particles that follows this distribution
        """
        mix_idx = torch.multinomial(self.mix_probs, n[0], replacement=True)
        means = self.means[mix_idx]
        stds = self.std[mix_idx]
        return torch.randn_like(means) * stds + means

    def log_prob(self, samples):
        """
        Given a tensor of particles, compute their log probability with respect to the current distribution
        (the mixtures are independent)
        Inputs:
            samples: A tensor of particles
        Outputs:
            A tensor of log probabilites
        """
        logps = []
        for i in range(len(self.mix_probs)):
            logps.append((-((samples.to(device) - self.means[i]) ** 2).sum(dim=-1) / (2 * self.sigma ** 2) - 0.5 * np.log(
                2 * np.pi * self.sigma ** 2)) + self.mix_probs[i].log())
        logp = torch.logsumexp(torch.stack(logps, dim=0), dim=0)
        return logp

"""# Optimizer"""

class Optim():
    """
    Optimizer Class
    Inputs:
        lr: Learning rate
    """
    def __init__(self, lr=None):
        self.m_dx, self.v_dx = 0, 0
        self.lr = lr

    def step(self,x, dx):
        """
        Gradient Ascent
        Inputs:
            x: A tensor of particles
            dx: The update vector
        Outputs:
            x: Updated particles
        """
        dx = dx.view(x.size())
        x = x + self.lr * dx
        return x

"""# Entropy Computation"""

class Entropy():
    """
    Entropy Class
    Inputs:
        P: GMM Object
        K: RBF Kernel
        optimizer: Gradient Ascent optimizer
        num_particles: Number of particles
        particles_dim: Particles' number of dimensions
    """
    def __init__(self, P, K, optimizer, num_particles, particles_dim):
        self.P = P
        self.optim = optimizer
        self.num_particles = num_particles
        self.particles_dim = particles_dim
        self.K = K


        # Mean of Langevin dynamics initial distribution
        mu_ld_noise = torch.zeros((self.particles_dim,))
        # Standard deviation of Langevin dynamics initial distribution
        sigma_ld_noise = torch.eye(self.particles_dim) * 0.1
        # Langevin dynamics initial distribution
        self.init_dist_ld = torch.distributions.MultivariateNormal(mu_ld_noise,covariance_matrix=sigma_ld_noise)
        # Identity matrix used in the computation of the entropy (line 1)
        self.identity_mat = torch.eye(self.particles_dim).to(device)
        # Identity matrix used in the computation of the entropy (line 3)
        self.identity_mat2 = 2 * torch.eye(self.num_particles).to(device)
        # Entropy Variables, Line 1,2 and 3
        self.logp_line1 = 0
        self.logp_line2 = 0
        self.logp_line3 = 0

    def SVGD(self,X):
        X = X.requires_grad_(True)
        log_prob = self.P.log_prob(X)

        self.score_func = autograd.grad(log_prob.sum(), X, retain_graph=True, create_graph=True)[0].reshape(self.num_particles, self.particles_dim)
        self.K_XX, self.K_grad, self.K_XX_phi, self.K_grad_phi = self.K.forward(X, X)

        curr_num_particles = num_particles

        self.phi_term1 = self.K_XX.matmul(self.score_func) / curr_num_particles
        self.phi_term2 = self.K_grad.sum(0) / curr_num_particles
        phi = self.phi_term1 + self.phi_term2

        phi_log_prob = (self.K_XX_phi).matmul(self.score_func) / (curr_num_particles-1)
        phi_log_prob += (self.K_grad_phi.sum(0) / (curr_num_particles-1))
        return phi, phi_log_prob, curr_num_particles


    def compute_logprob(self, phi, X):
        grad_phi =[]
        for i in range(len(X)):
            grad_phi_tmp = []
            for j in range(self.particles_dim):
                grad_ = autograd.grad(phi[i][j], X, retain_graph=True)[0][i].detach()
                grad_phi_tmp.append(grad_)
            grad_phi.append(torch.stack(grad_phi_tmp))
        self.grad_phi = torch.stack(grad_phi)

        self.logp_line1 = self.logp_line1 - torch.log(torch.abs(torch.det(self.identity_mat + self.optim.lr * self.grad_phi)))

        grad_phi_trace = torch.stack( [torch.trace(grad_phi[i]) for i in range(len(grad_phi))] )
        self.logp_line2 = self.logp_line2 - self.optim.lr * grad_phi_trace


    def step(self, X, V, alg):
        phi_X, phi_X_log_prob, particles_active = self.SVGD(X)
        X_new = self.optim.step(X, phi_X)
        self.phi_X = phi_X
        self.compute_logprob(phi_X_log_prob, X)
        convergence = ((X_new-X)**2).sum(-1).mean()
        svgd_converegence = torch.mean((phi_X)**2)
        X = X_new.detach()

        return X, phi_X, convergence, svgd_converegence, particles_active

"""# Experiment"""

def my_experiment(dim, n_gmm, num_particles, num_steps, gmm_std, lr, mu_init, sigma_init, sigma_V_init, dist_type, ):
    mu = torch.full((dim,), mu_init).to(device).to(torch.float32)
    sigma = (torch.eye(dim) * sigma_init).to(device)
    init_dist = torch.distributions.MultivariateNormal(mu,covariance_matrix=sigma)
    X_init = init_dist.sample((num_particles,)).to(device)

    init_V_dist = torch.distributions.MultivariateNormal(torch.zeros(dim).to(device),covariance_matrix = (torch.eye(dim) * sigma_V_init).to(device))
    V_init = init_V_dist.sample((num_particles,)).to(device)

    if dist_type == 'gmm':
        # Figures 4b, high dimensionality figures
        dist = GMMDist(dim=dim, n_gmm=n_gmm, sigma=gmm_std)
    elif dist_type == 'gauss':
        # Figure 4a
        dist = torch.distributions.MultivariateNormal(torch.Tensor([-0.6871,0.8010]).to(device),covariance_matrix= 5 *torch.Tensor([[0.2260,0.1652],[0.1652,0.6779]]).to(device))

    # Create an entropy experiment object
    experiment = Entropy(dist, Bilinear_kernel(dim, num_particles), Optim(lr), num_particles=num_particles, particles_dim=dim)

    # Generate plots only if dim == 2
    if dim == 2:
        gauss_chart = get_density_chart(dist, d=7.0, step=0.1, dist_type=dist_type)
        init_chart = gauss_chart + get_particles_chart(X_init.cpu().numpy())
    else:
        init_chart = None

    # Compute the groud truth entropy
    #gt_entr = - dist.log_prob(dist.sample((10000,))).mean().cpu().item()
    gt_entr = dist.entropy().detach().cpu()

    def main_loop(alg, X, V, steps):
        """
        Perform SVGD updates
        Inputs:
            X: A tensor of particles
            steps: The number of SVGD steps
        Outputs:
            sampler_entr_svgd: The SVGD Entropy
            gt_entr: The ground truth Entropy
            charts: The list of density plots with particles.(a plot for each svgd step)
        """
        # gamma = 1.0 / (1e-8 + 2 * sigma**2)
        active_particles = []
        entropies = []
        convergence_values = []
        charts = []
        X_svgd_=[]
        for t in tqdm(range(steps)):
            # Perform one update step
            X, phi_X, conv, svgd_conv, active = experiment.step(X, V, alg)
            # Save the intermidiate position of the particles
            X_svgd_.append(X.clone())
            # Generate plots only if dim == 2
            entropy_l_1 =  -(init_dist.log_prob(X_init) + experiment.logp_line1).mean().item()
            entropy_l_2 =  -(init_dist.log_prob(X_init) + experiment.logp_line2).mean().item()

            neighbor_avg = (((experiment.K_XX > 0.01).int()).sum(-1)).float().mean()
            active_particles.append(active)

            if dim == 2:
                chart_ = gauss_chart + get_particles_chart(X.detach().cpu().numpy())
                charts.append(chart_)

            convergence_values.append(svgd_conv)
            entropies.append([entropy_l_1, entropy_l_2])

        X_svgd_ = torch.stack(X_svgd_)
        convergence_values = torch.stack(convergence_values).cpu().detach().numpy()

        # Compute the entropy of the particles using the log probability
        sampler_entr =  -(init_dist.log_prob(X_init) + experiment.logp_line2).mean().item()

        print('################# sampler_entr: ', alg, sampler_entr)

        return sampler_entr, charts, convergence_values , entropies,  active_particles


    sampler_entr_svgd, charts_svgd, conv, ent, act = main_loop('svgd', X_init.clone().detach(), None, steps=num_steps)
    return sampler_entr_svgd, charts_svgd, conv, ent, gt_entr,  act

"""# Reproduce Paper Figures

### Figure 4a
"""
torch.manual_seed(12)
dist_type = 'gauss'
dim = 2
lr = 0.5
n_gmm = 1
gmm_std = 1
num_particles = 400
num_steps = 1000
kernel_variance = 5 #'forth'
sigma_V_init = 5

mu_init = 0
sigma_init = 6
sampler_entr_svgd, charts_svgd, conv, ent, gt_entr,  act = my_experiment(dim, n_gmm, num_particles, num_steps, gmm_std, lr, mu_init, sigma_init, sigma_V_init, dist_type,)
name = 'bilinear_kernel'

charts_svgd[0]

charts_svgd[-1]

charts = charts_svgd

qualitative_conv = charts[0].properties(title='init step')
for i in range(100, num_steps, 100):
    qualitative_conv |= charts[i].properties(title=f'step {i}')
qualitative_conv |= charts[-1].properties(title=f'step {num_steps}')
qualitative_conv.save('./' +name+ '.html') #add chart directory

qualitative_conv

plt.clf()
ent1 = [ent[i][0] for i in range(len(ent))]
ent2 = [ent[i][1] for i in range(len(ent))]

df = pd.DataFrame({'svgd_step':np.arange(num_steps), 'line1':ent1, 'line2':ent2})
## gt entropy
plt.axhline(y = gt_entr, color='r', linestyle='-', label = 'gt_entropy')
# line 1
plt.plot(df['svgd_step'], df['line1'], label = 'line 1', color = '#006400')
#line 2
plt.plot(df['svgd_step'], df['line2'], label = 'line 2', color = '#00CED1')
# add extra axis for the stein id
# plt.plot(stein_id_0['svgd_step'], stein_id_0['stein identity'], color= '#0000FF')
plt.legend()

plt.xlabel('itr')
plt.ylabel('entropies')
plt.savefig(name+'.png')
plt.show()
